# Quantizing-Convolutional-Neural-Networks-for-Practical-Deployment
Quantization is vital in edge AI, converting high-precision floating-point numbers to fixed-point or integers. This reduces memory demands, enabling efficient processing on resource-constrained devices while maintaining acceptable accuracy. 

# Quantization levels
![quantization_vis](https://github.com/cyndwith/Quantizing-Convolutional-Neural-Networks-for-Practical-Deployment/assets/11755434/ccd34f3f-ba10-4f1b-866d-1acc7a5e2cb2)


# Calibration dataset
![newplot (1)](https://github.com/cyndwith/Quantizing-Convolutional-Neural-Networks-for-Practical-Deployment/assets/11755434/508ce74b-afa3-447e-a0b3-d34bdc0a4fe1)

# References
[1] https://www.edge-ai-vision.com/2023/12/from-theory-to-practice-quantizing-convolutional-neural-networks-for-practical-deployment/

[2] https://www.edge-ai-vision.com/2024/02/quantization-of-convolutional-neural-networks-model-quantization/

[3] Dwith Chenna, “Quantization of Convolutional Neural Networks: A Practical Approach”, International Journal of Science & Engineering Development Research, Vol.8, Issue 12, page no.181 – 192, December-2023, Available :http://www.ijrti.org/papers/IJRTI2312025.pdf
